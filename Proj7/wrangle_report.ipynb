{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document describes the wrangling efforts needed and the approach taken to munge the raw data into a form from which visualizations and analysis can be generated. The steps taken for data wrangling are - Gathering, Assessing and Cleaning, in that order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is gathered from three sources- by manually downloading the `twitter_archive_enhanced.csv` flat-file, by programmatically downloading the `image_predictions.tsv` file from Udacity's servers, and by creating the `tweet_json.txt` text file by querying the complete tweet JSON for each of the tweet_ids in twitter_archive_enhanced.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter API credentials for tweepy library are read from a hidden file named `.twitter_api_credentials.json` that has the following JSON-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "\"consumer_key\": \"YOUR_CONSUMER_KEY\",\n",
    "\"consumer_secret\": \"YOUR_CONSUMER_SECRET\",\n",
    "\"access_token\": \"YOUR_ACCESS_TOKEN\",\n",
    "\"access_secret\": \"YOUR_ACCESS_SECRET\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then `tweet_json.txt` file is read back and metadata info about the tweet is imported into a dataframe containing the following columns- `tweet_id`, `retweet_count`, and `favorite_count` since these appear to be the ones that most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a combination of directed and non-directed assessment. For directed assessment, we need to decide on what questions we want this dataset to answer, so that we can look for columns and rows of interest to that end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that this dataset should be able to help us answer the following questions using visualizations and stats-\n",
    "- Doggie demographics on WeRateDogs- most commonly tweeted about dog stage, breed etc\n",
    "- Factors that lead to higher ratings- is it influenced by dog category or (predicted) dog breed.\n",
    "- Factors that encourage user engagement- do higher retweets and favorites correlate with higher ratings or is it skewed by factors such as day of posting (do weekend posts get higher engagement?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that are most relevant to this are - prediction columns in image dataframe, `timestamp`, retweet and favorite counts, `rating_numerator`, `rating_denominator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do a quick visual inspection of all the datasets before proceeding on with programmatic assessments. We make a note of these issues in the Summary section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we begin with .info() on all methods, referring to WeRateDogs twitter page to understand their schema, and to the twitter API documentation (https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html) for details of twitter json. For dog predictions, we had to decide on a heuristic and confidence level at which we accept the prediction (we set it at 50%). We also, keep the twitter id's exactly the same in all the dataframes to make sure we're answering all the questions using a consistent set of datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed with programmatic assessment looking for quality issues using the thumbrule of looking for `Completeness`, `Validity`, `Accuracy` and `Consistency` issues and also note these under the summary section under each table. While doing so, we also notice `tidiness` issues which become further apparent when we try to store the data and use it for analysis. We revisit this list several times, during cleaning and analysis- iterating and updating the order so that we don't have to clean data that is not useful for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start with cleaning for Missing data or `Completeness` issues, and then clean for `Tidiness`. After this we proceed to clean other quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to `Define` what programmatic steps need to be taken for cleaning, followed by the actual `Code` and finally `Test` that the cleaning was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use assertions and visual assessment, including printing out complete tweet text and clicking on tweet link to look at the dog type etc. to aid in defining how we should handle quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk of the cleaning involved reparsing the ratings and the dog_stages and predictions apart from datatype cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the above process, and keeping the end analysis goal and relevant columns in mind, we iterate as necessary until we munge the data enough to be able to answer the questions we set out to answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
